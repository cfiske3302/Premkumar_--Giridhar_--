{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "pygame 2.5.2 (SDL 2.28.3, Python 3.10.13)\n",
                        "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
                    ]
                }
            ],
            "source": [
                "import numpy as np\n",
                "import sys\n",
                "import matplotlib.pyplot as plt\n",
                "import gym\n",
                "\n",
                "sys.path.insert(0, './TetrisEnv')\n",
                "from TetrisBattle.envs.tetris_env import TetrisSingleEnv, TetrisEnv\n",
                "from TetrisBattle import *\n",
                "from Model.Feature_Extraction import TetrisActorCriticCnnPolicy\n",
                "# %pip install -e ./TetrisEnv\n",
                "# import TetrisBattle"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "%load_ext autoreload\n",
                "%autoreload 2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class InputGetter():\n",
                "    def __init__(self):\n",
                "        self.input_log = []\n",
                "        \n",
                "    def get_input(self):\n",
                "        while True:\n",
                "            try:\n",
                "                if len(self.input_log) == 0:\n",
                "                    self.input_log = [*input(\">\")]\n",
                "                \n",
                "                inp = self.input_log.pop(0)\n",
                "                inp = int(inp)\n",
                "                assert(inp in [0, 1, 2, 3, 4, 5, 6, 7, 9])\n",
                "                return inp\n",
                "            except Exception as e:\n",
                "                print(e)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "env = TetrisSingleEnv(obs_type=\"grid\")\n",
                "i = 0\n",
                "inputter = InputGetter()\n",
                "while True:\n",
                "    action = inputter.get_input()\n",
                "    if action == 9:\n",
                "        env.close()\n",
                "        break\n",
                "    state, reward, done, info = env.step(action)\n",
                "    # print(\"infow\", info)\n",
                "    # print(\"rewrd\",reward)\n",
                "    env.render()\n",
                "    # print(env.game_interface.get_obs().shape)\n",
                "    grid = env.game_interface.get_obs()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "grid[:,:,0]\n",
                "print(grid.shape)\n",
                "bool_grid = grid != 0\n",
                "for i in range(len(grid)):\n",
                "    for j in range(len(grid[0,:15])):\n",
                "        num = float(grid[i,j,0])\n",
                "        # print(f\"{bool_grid[i,j,0]}, \", end=\"\")\n",
                "        print(f\"{round(num, 1)}, \", end=\"\")\n",
                "    print()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "env = TetrisSingleEnv()\n",
                "i = 0\n",
                "inputter = InputGetter()\n",
                "while True:\n",
                "    action = inputter.get_input()\n",
                "    if action == 9:\n",
                "        env.close()\n",
                "        break\n",
                "    state, reward, done, info = env.og_step(action)\n",
                "    print(\"infow\", info)\n",
                "    print(\"rewrd\",reward)\n",
                "    env.render()\n",
                "    print(env.get_obs())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "# env = make('SinglePTetris-v0')\n",
                "env = TetrisSingleEnv()\n",
                "ob = env.reset()\n",
                "dots = np.ones((20, 10))\n",
                "x = 100\n",
                "y = 130\n",
                "ob[x:x+10,y:y+10] = [255,255,255]\n",
                "plt.imshow(ob, cmap=\"plasma\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    assert(False)\n",
                "except:\n",
                "    pass"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "libpng warning: iCCP: known incorrect sRGB profile\n",
                        "libpng warning: iCCP: known incorrect sRGB profile\n",
                        "libpng warning: iCCP: known incorrect sRGB profile\n",
                        "libpng warning: iCCP: known incorrect sRGB profile\n",
                        "libpng warning: iCCP: known incorrect sRGB profile\n",
                        "libpng warning: iCCP: known incorrect sRGB profile\n",
                        "libpng warning: iCCP: known incorrect sRGB profile\n",
                        "libpng warning: iCCP: known incorrect sRGB profile\n",
                        "libpng warning: iCCP: known incorrect sRGB profile\n",
                        "libpng warning: iCCP: known incorrect sRGB profile\n",
                        "libpng warning: iCCP: known incorrect sRGB profile\n",
                        "libpng warning: iCCP: known incorrect sRGB profile\n",
                        "libpng warning: iCCP: known incorrect sRGB profile\n",
                        "libpng warning: iCCP: known incorrect sRGB profile\n",
                        "libpng warning: iCCP: known incorrect sRGB profile\n",
                        "libpng warning: iCCP: known incorrect sRGB profile\n",
                        "libpng warning: iCCP: known incorrect sRGB profile\n",
                        "/opt/miniconda3/envs/ECE_113_Capstone_1/lib/python3.10/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
                        "  warnings.warn(\n"
                    ]
                }
            ],
            "source": [
                "from gym.wrappers import GrayScaleObservation\n",
                "from preprocessing.EensyWeensy import MakeEensyWeensy\n",
                "from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv\n",
                "# from gym import make\n",
                "\n",
                "naked_env = TetrisSingleEnv(obs_type=\"grid\")\n",
                "# env = GrayScaleObservation(env, keep_dim=True)\n",
                "# env = MakeEensyWeensy(env, cut_in_half=True, scale=.25)\n",
                "env = DummyVecEnv([lambda: naked_env])\n",
                "# env = VecFrameStack(env, 4, channels_order=\"last\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# from Model.Feature_Extraction import TetisFeatureExtractor\n",
                "# feat_ex = TetisFeatureExtractor(gym.spaces.Box(0.0, 1.0, (20, 34), \"float32\"))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ob,_,_,_ = env.step([0])\n",
                "ob.transpose(3, 0, 1, 2).shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# for i in range(0, 1000):\n",
                "#     action = env.random_action()\n",
                "#     state, reward, done, info = env.step(action)\n",
                "#     env.render()\n",
                "#     i += 1\n",
                "# env.close()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "False\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "libpng warning: iCCP: known incorrect sRGB profile\n",
                        "libpng warning: iCCP: known incorrect sRGB profile\n",
                        "libpng warning: iCCP: known incorrect sRGB profile\n",
                        "libpng warning: iCCP: known incorrect sRGB profile\n",
                        "libpng warning: iCCP: known incorrect sRGB profile\n",
                        "libpng warning: iCCP: known incorrect sRGB profile\n",
                        "libpng warning: iCCP: known incorrect sRGB profile\n",
                        "libpng warning: iCCP: known incorrect sRGB profile\n",
                        "libpng warning: iCCP: known incorrect sRGB profile\n",
                        "libpng warning: iCCP: known incorrect sRGB profile\n",
                        "libpng warning: iCCP: known incorrect sRGB profile\n",
                        "libpng warning: iCCP: known incorrect sRGB profile\n",
                        "libpng warning: iCCP: known incorrect sRGB profile\n",
                        "libpng warning: iCCP: known incorrect sRGB profile\n",
                        "libpng warning: iCCP: known incorrect sRGB profile\n",
                        "libpng warning: iCCP: known incorrect sRGB profile\n",
                        "libpng warning: iCCP: known incorrect sRGB profile\n"
                    ]
                },
                {
                    "ename": "AssertionError",
                    "evalue": "Your environment must inherit from the gymnasium.Env class cf. https://gymnasium.farama.org/api/env/",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available())\n\u001b[0;32m----> 7\u001b[0m \u001b[43mcheck_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTetrisSingleEnv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m/opt/miniconda3/envs/ECE_113_Capstone_1/lib/python3.10/site-packages/stable_baselines3/common/env_checker.py:409\u001b[0m, in \u001b[0;36mcheck_env\u001b[0;34m(env, warn, skip_render_check)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_env\u001b[39m(env: gym\u001b[38;5;241m.\u001b[39mEnv, warn: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, skip_render_check: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    395\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;124;03m    Check that an environment follows Gym API.\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;124;03m    This is particularly useful when using a custom environment.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;124;03m        True by default (useful for the CI)\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 409\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    410\u001b[0m         env, gym\u001b[38;5;241m.\u001b[39mEnv\n\u001b[1;32m    411\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour environment must inherit from the gymnasium.Env class cf. https://gymnasium.farama.org/api/env/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;66;03m# ============= Check the spaces (observation and action) ================\u001b[39;00m\n\u001b[1;32m    414\u001b[0m     _check_spaces(env)\n",
                        "\u001b[0;31mAssertionError\u001b[0m: Your environment must inherit from the gymnasium.Env class cf. https://gymnasium.farama.org/api/env/"
                    ]
                }
            ],
            "source": [
                "import os \n",
                "from stable_baselines3 import PPO\n",
                "from stable_baselines3.common.callbacks import BaseCallback\n",
                "from stable_baselines3.common.env_checker import check_env\n",
                "import torch\n",
                "print(torch.cuda.is_available())\n",
                "check_env(TetrisSingleEnv())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "class TrainAndLoggingCallback(BaseCallback):\n",
                "\n",
                "    def __init__(self, check_freq, save_path, verbose=1):\n",
                "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
                "        self.check_freq = check_freq\n",
                "        self.save_path = save_path\n",
                "\n",
                "    def _init_callback(self):\n",
                "        if self.save_path is not None:\n",
                "            os.makedirs(self.save_path, exist_ok=True)\n",
                "\n",
                "    def _on_step(self):\n",
                "        if self.n_calls % self.check_freq == 0:\n",
                "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
                "            self.model.save(model_path)\n",
                "\n",
                "        return True"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "CHECKPOINT_DIR = './train/'\n",
                "LOG_DIR = './logs/'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "callback = TrainAndLoggingCallback(check_freq=10000, save_path=CHECKPOINT_DIR)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using cpu device\n",
                        "torch.Size([1, 1, 20, 34, 1])\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/opt/miniconda3/envs/ECE_113_Capstone_1/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:155: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 32`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 32\n",
                        "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
                        "Info: (n_steps=32 and n_envs=1)\n",
                        "  warnings.warn(\n"
                    ]
                }
            ],
            "source": [
                "try:\n",
                "    del model\n",
                "except NameError:\n",
                "    pass\n",
                "model = PPO(TetrisActorCriticCnnPolicy, env, verbose=1, tensorboard_log=LOG_DIR, learning_rate=1e-2, n_steps=32, device=\"cuda\") \n",
                "# TetrisActorCriticCnnPolicy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Logging to ./logs/PPO_1\n",
                        "---------------------------------\n",
                        "| rollout/           |          |\n",
                        "|    ep_len_mean     | 12.5     |\n",
                        "|    ep_rew_mean     | -54.3    |\n",
                        "| time/              |          |\n",
                        "|    fps             | 17       |\n",
                        "|    iterations      | 1        |\n",
                        "|    time_elapsed    | 1        |\n",
                        "|    total_timesteps | 32       |\n",
                        "---------------------------------\n",
                        "----------------------------------------\n",
                        "| rollout/                |            |\n",
                        "|    ep_len_mean          | 13.2       |\n",
                        "|    ep_rew_mean          | -57.4      |\n",
                        "| time/                   |            |\n",
                        "|    fps                  | 14         |\n",
                        "|    iterations           | 2          |\n",
                        "|    time_elapsed         | 4          |\n",
                        "|    total_timesteps      | 64         |\n",
                        "| train/                  |            |\n",
                        "|    approx_kl            | 0.21038488 |\n",
                        "|    clip_fraction        | 0.722      |\n",
                        "|    clip_range           | 0.2        |\n",
                        "|    entropy_loss         | -3.6       |\n",
                        "|    explained_variance   | 0.00654    |\n",
                        "|    learning_rate        | 0.01       |\n",
                        "|    loss                 | 158        |\n",
                        "|    n_updates            | 10         |\n",
                        "|    policy_gradient_loss | -0.111     |\n",
                        "|    value_loss           | 430        |\n",
                        "----------------------------------------\n",
                        "----------------------------------------\n",
                        "| rollout/                |            |\n",
                        "|    ep_len_mean          | 13         |\n",
                        "|    ep_rew_mean          | -55        |\n",
                        "| time/                   |            |\n",
                        "|    fps                  | 15         |\n",
                        "|    iterations           | 3          |\n",
                        "|    time_elapsed         | 6          |\n",
                        "|    total_timesteps      | 96         |\n",
                        "| train/                  |            |\n",
                        "|    approx_kl            | 0.37260336 |\n",
                        "|    clip_fraction        | 0.622      |\n",
                        "|    clip_range           | 0.2        |\n",
                        "|    entropy_loss         | -3.43      |\n",
                        "|    explained_variance   | 5.96e-08   |\n",
                        "|    learning_rate        | 0.01       |\n",
                        "|    loss                 | 187        |\n",
                        "|    n_updates            | 20         |\n",
                        "|    policy_gradient_loss | -0.0882    |\n",
                        "|    value_loss           | 481        |\n",
                        "----------------------------------------\n"
                    ]
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m/opt/miniconda3/envs/ECE_113_Capstone_1/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m/opt/miniconda3/envs/ECE_113_Capstone_1/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:277\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 277\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
                        "File \u001b[0;32m/opt/miniconda3/envs/ECE_113_Capstone_1/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:194\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m         \u001b[38;5;66;03m# Otherwise, clip the actions to avoid out of bound error\u001b[39;00m\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;66;03m# as we are sampling from an unbounded Gaussian distribution\u001b[39;00m\n\u001b[1;32m    192\u001b[0m         clipped_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[0;32m--> 194\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
                        "File \u001b[0;32m/opt/miniconda3/envs/ECE_113_Capstone_1/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m/opt/miniconda3/envs/ECE_113_Capstone_1/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m---> 58\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
                        "File \u001b[0;32m/opt/miniconda3/envs/ECE_113_Capstone_1/lib/python3.10/site-packages/shimmy/openai_gym_compatibility.py:251\u001b[0m, in \u001b[0;36mGymV21CompatibilityV0.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: ActType) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[Any, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m]:\n\u001b[1;32m    243\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \n\u001b[1;32m    245\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;124;03m        (observation, reward, terminated, truncated, info)\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m     obs, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgym_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender()\n",
                        "File \u001b[0;32m~/Desktop/capstone/ECE113_capstone/./TetrisEnv/TetrisBattle/envs/tetris_env.py:127\u001b[0m, in \u001b[0;36mTetrisSingleEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    124\u001b[0m     ac \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# if self.since_last_drop >= 64:\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m#     ac = 2\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m ob, step_reward, end, new_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgame_interface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mac\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m step_reward\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# if 'height_sum' in infos:\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m#     # print(infos)\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m#     reward -= infos['height_sum'] * 0.2\u001b[39;00m\n",
                        "File \u001b[0;32m~/Desktop/capstone/ECE113_capstone/./TetrisEnv/TetrisBattle/envs/tetris_interface.py:503\u001b[0m, in \u001b[0;36mTetrisSingleInterface.act\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrenderer\u001b[38;5;241m.\u001b[39mdrawKO(tetris\u001b[38;5;241m.\u001b[39mKO, pos[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbig_ko\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m], pos[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbig_ko\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrenderer\u001b[38;5;241m.\u001b[39mdrawScreen(tetris, pos[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrawscreen\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m], pos[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrawscreen\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrenderer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrawByName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtransparent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mopponent\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpos\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtransparent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_time(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime)\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
                        "File \u001b[0;32m~/Desktop/capstone/ECE113_capstone/./TetrisEnv/TetrisBattle/renderer.py:310\u001b[0m, in \u001b[0;36mRenderer.drawByName\u001b[0;34m(self, name, sx, sy)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrawByName\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, sx, sy):\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscreen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msy\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "source": [
                "model.learn(total_timesteps=100000, callback=callback)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "libpng warning: iCCP: known incorrect sRGB profile\n",
                        "libpng warning: iCCP: known incorrect sRGB profile\n",
                        "libpng warning: iCCP: known incorrect sRGB profile\n",
                        "libpng warning: iCCP: known incorrect sRGB profile\n",
                        "libpng warning: iCCP: known incorrect sRGB profile\n",
                        "libpng warning: iCCP: known incorrect sRGB profile\n",
                        "libpng warning: iCCP: known incorrect sRGB profile\n",
                        "libpng warning: iCCP: known incorrect sRGB profile\n",
                        "libpng warning: iCCP: known incorrect sRGB profile\n",
                        "libpng warning: iCCP: known incorrect sRGB profile\n",
                        "libpng warning: iCCP: known incorrect sRGB profile\n",
                        "libpng warning: iCCP: known incorrect sRGB profile\n",
                        "libpng warning: iCCP: known incorrect sRGB profile\n",
                        "libpng warning: iCCP: known incorrect sRGB profile\n",
                        "libpng warning: iCCP: known incorrect sRGB profile\n",
                        "libpng warning: iCCP: known incorrect sRGB profile\n",
                        "libpng warning: iCCP: known incorrect sRGB profile\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "reward -0.72\n",
                        "False\n",
                        "reward -1.44\n",
                        "False\n",
                        "reward -4.2\n",
                        "False\n",
                        "reward -0.36\n",
                        "False\n",
                        "reward -9.42\n",
                        "False\n",
                        "reward -8.19\n",
                        "False\n",
                        "reward -6.09\n",
                        "False\n",
                        "reward -2.46\n",
                        "False\n",
                        "reward -1.9500000000000002\n",
                        "False\n",
                        "reward -5.94\n",
                        "False\n",
                        "reward -16.669999999999998\n",
                        "True\n",
                        "reward -4.34\n",
                        "True\n",
                        "reward -3.96\n",
                        "True\n",
                        "reward -3.96\n",
                        "True\n",
                        "reward -3.96\n",
                        "True\n",
                        "reward -3.96\n",
                        "True\n",
                        "reward -11.86\n",
                        "True\n",
                        "reward -4.96\n",
                        "True\n",
                        "reward -4.96\n",
                        "True\n",
                        "reward -14.36\n",
                        "True\n",
                        "reward -4.96\n",
                        "True\n",
                        "reward -4.96\n",
                        "True\n",
                        "reward -5.96\n",
                        "True\n",
                        "reward -5.96\n",
                        "True\n",
                        "reward -5.96\n",
                        "True\n",
                        "reward -5.96\n",
                        "True\n",
                        "reward -5.96\n",
                        "True\n",
                        "reward -5.96\n",
                        "True\n",
                        "reward -5.96\n",
                        "True\n",
                        "reward -5.96\n",
                        "True\n",
                        "reward -5.96\n",
                        "True\n",
                        "reward -5.96\n",
                        "True\n",
                        "reward -6.96\n",
                        "True\n",
                        "reward -6.96\n",
                        "True\n",
                        "reward -6.96\n",
                        "True\n",
                        "reward -6.96\n",
                        "True\n",
                        "reward -6.96\n",
                        "True\n",
                        "reward -6.96\n",
                        "True\n",
                        "reward -6.96\n",
                        "True\n",
                        "reward -6.96\n",
                        "True\n",
                        "reward -6.96\n",
                        "True\n",
                        "reward -19.7\n",
                        "True\n",
                        "reward -7.96\n",
                        "True\n",
                        "reward -7.96\n",
                        "True\n",
                        "reward -24.39\n",
                        "True\n",
                        "reward -8.96\n",
                        "True\n",
                        "reward -24.520000000000003\n",
                        "True\n",
                        "reward -8.96\n",
                        "True\n",
                        "reward -23.04\n",
                        "True\n",
                        "reward -9.96\n",
                        "True\n",
                        "reward -9.96\n",
                        "True\n",
                        "reward -9.96\n",
                        "True\n",
                        "reward -9.96\n",
                        "True\n",
                        "reward -9.96\n",
                        "True\n",
                        "reward -9.96\n",
                        "True\n",
                        "reward -9.96\n",
                        "True\n",
                        "reward -9.96\n",
                        "True\n",
                        "reward -10.96\n",
                        "True\n",
                        "reward -10.96\n",
                        "True\n",
                        "reward -10.96\n",
                        "True\n",
                        "reward -10.96\n",
                        "True\n",
                        "reward -10.96\n",
                        "True\n",
                        "reward -10.96\n",
                        "True\n",
                        "reward -10.96\n",
                        "True\n",
                        "reward -10.96\n",
                        "True\n",
                        "reward -10.96\n",
                        "True\n",
                        "reward -10.96\n",
                        "True\n",
                        "reward -11.96\n",
                        "True\n",
                        "reward -11.96\n",
                        "True\n",
                        "reward -11.96\n",
                        "True\n",
                        "reward -11.96\n",
                        "True\n",
                        "reward -11.96\n",
                        "True\n",
                        "reward -11.96\n",
                        "True\n",
                        "reward -11.96\n",
                        "True\n",
                        "reward -11.96\n",
                        "True\n",
                        "reward -11.96\n",
                        "True\n",
                        "reward -11.96\n",
                        "True\n",
                        "reward -12.96\n",
                        "True\n",
                        "reward -12.96\n",
                        "True\n",
                        "reward -12.96\n",
                        "True\n",
                        "reward -12.96\n",
                        "True\n",
                        "reward -12.96\n",
                        "True\n",
                        "reward -12.96\n",
                        "True\n",
                        "reward -12.96\n",
                        "True\n",
                        "reward -12.96\n",
                        "True\n",
                        "reward -12.96\n",
                        "True\n",
                        "reward -12.96\n",
                        "True\n",
                        "reward -13.96\n",
                        "True\n",
                        "reward -13.96\n",
                        "True\n",
                        "reward -34.55\n",
                        "True\n",
                        "reward -13.96\n",
                        "True\n",
                        "reward -13.96\n",
                        "True\n",
                        "reward -13.96\n",
                        "True\n",
                        "reward -13.96\n",
                        "True\n",
                        "reward -14.96\n",
                        "True\n",
                        "reward -14.96\n",
                        "True\n",
                        "reward -14.96\n",
                        "True\n",
                        "reward -14.96\n",
                        "True\n",
                        "reward -14.96\n",
                        "True\n",
                        "reward -14.96\n",
                        "True\n",
                        "reward -14.96\n",
                        "True\n",
                        "reward -14.96\n",
                        "True\n",
                        "reward -14.96\n",
                        "True\n",
                        "reward -14.96\n",
                        "True\n",
                        "reward -15.96\n",
                        "True\n",
                        "reward -15.96\n",
                        "True\n",
                        "reward -15.96\n",
                        "True\n",
                        "reward -15.96\n",
                        "True\n",
                        "reward -15.96\n",
                        "True\n",
                        "reward -15.96\n",
                        "True\n",
                        "reward -15.96\n",
                        "True\n",
                        "reward -15.96\n",
                        "True\n",
                        "reward -15.96\n",
                        "True\n",
                        "reward -15.96\n",
                        "True\n",
                        "reward -16.96\n",
                        "True\n",
                        "reward -16.96\n",
                        "True\n",
                        "reward -16.96\n",
                        "True\n",
                        "reward -16.96\n",
                        "True\n",
                        "reward -16.96\n",
                        "True\n",
                        "reward -16.96\n",
                        "True\n",
                        "reward -16.96\n",
                        "True\n",
                        "reward -16.96\n",
                        "True\n",
                        "reward -16.96\n",
                        "True\n",
                        "reward -16.96\n",
                        "True\n",
                        "reward -17.96\n",
                        "True\n",
                        "reward -17.96\n",
                        "True\n",
                        "reward -17.96\n",
                        "True\n",
                        "reward -17.96\n",
                        "True\n",
                        "reward -17.96\n",
                        "True\n",
                        "reward -17.96\n",
                        "True\n",
                        "reward -17.96\n",
                        "True\n",
                        "reward -17.96\n",
                        "True\n",
                        "reward -17.96\n",
                        "True\n",
                        "reward -17.96\n",
                        "True\n",
                        "reward -18.96\n",
                        "True\n",
                        "reward -18.96\n",
                        "True\n",
                        "reward -18.96\n",
                        "True\n",
                        "reward -18.96\n",
                        "True\n",
                        "reward -18.96\n",
                        "True\n",
                        "reward -18.96\n",
                        "True\n",
                        "reward -18.96\n",
                        "True\n",
                        "reward -18.96\n",
                        "True\n",
                        "reward -18.96\n",
                        "True\n",
                        "reward -18.96\n",
                        "True\n",
                        "reward -19.96\n",
                        "True\n",
                        "reward -19.96\n",
                        "True\n",
                        "reward -19.96\n",
                        "True\n",
                        "reward -19.96\n",
                        "True\n",
                        "reward -19.96\n",
                        "True\n",
                        "reward -19.96\n",
                        "True\n",
                        "reward -19.96\n",
                        "True\n",
                        "reward -19.96\n",
                        "True\n",
                        "reward -19.96\n",
                        "True\n",
                        "reward -19.96\n",
                        "True\n",
                        "reward -20.96\n",
                        "True\n",
                        "reward -20.96\n",
                        "True\n",
                        "reward -20.96\n",
                        "True\n",
                        "reward -20.96\n",
                        "True\n",
                        "reward -20.96\n",
                        "True\n",
                        "reward -20.96\n",
                        "True\n",
                        "reward -20.96\n",
                        "True\n",
                        "reward -20.96\n",
                        "True\n",
                        "reward -20.96\n",
                        "True\n",
                        "reward -20.96\n",
                        "True\n",
                        "reward -21.96\n",
                        "True\n",
                        "reward -21.96\n",
                        "True\n",
                        "reward -21.96\n",
                        "True\n",
                        "reward -21.96\n",
                        "True\n",
                        "reward -21.96\n",
                        "True\n",
                        "reward -21.96\n",
                        "True\n",
                        "reward -21.96\n",
                        "True\n",
                        "reward -21.96\n",
                        "True\n",
                        "reward -21.96\n",
                        "True\n",
                        "reward -21.96\n",
                        "True\n",
                        "reward -22.96\n",
                        "True\n",
                        "reward -22.96\n",
                        "True\n",
                        "reward -22.96\n",
                        "True\n",
                        "reward -22.96\n",
                        "True\n",
                        "reward -22.96\n",
                        "True\n",
                        "reward -22.96\n",
                        "True\n",
                        "reward -22.96\n",
                        "True\n",
                        "reward -22.96\n",
                        "True\n",
                        "reward -22.96\n",
                        "True\n",
                        "reward -22.96\n",
                        "True\n",
                        "reward -23.96\n",
                        "True\n",
                        "reward -23.96\n",
                        "True\n",
                        "reward -23.96\n",
                        "True\n",
                        "reward -23.96\n",
                        "True\n",
                        "reward -23.96\n",
                        "True\n",
                        "reward -23.96\n",
                        "True\n",
                        "reward -23.96\n",
                        "True\n",
                        "reward -23.96\n",
                        "True\n",
                        "reward -23.96\n",
                        "True\n",
                        "reward -23.96\n",
                        "True\n",
                        "reward -24.96\n",
                        "True\n",
                        "reward -24.96\n",
                        "True\n",
                        "reward -24.96\n",
                        "True\n",
                        "reward -24.96\n",
                        "True\n",
                        "reward -24.96\n",
                        "True\n",
                        "reward -24.96\n",
                        "True\n",
                        "reward -24.96\n",
                        "True\n",
                        "reward -24.96\n",
                        "True\n",
                        "reward -24.96\n",
                        "True\n",
                        "reward -24.96\n",
                        "True\n",
                        "reward -25.96\n",
                        "True\n",
                        "reward -25.96\n",
                        "True\n",
                        "reward -25.96\n",
                        "True\n",
                        "reward -25.96\n",
                        "True\n",
                        "reward -25.96\n",
                        "True\n",
                        "reward -25.96\n",
                        "True\n",
                        "reward -25.96\n",
                        "True\n",
                        "reward -25.96\n",
                        "True\n",
                        "reward -25.96\n",
                        "True\n",
                        "reward -25.96\n",
                        "True\n",
                        "reward -26.96\n",
                        "True\n",
                        "reward -26.96\n",
                        "True\n",
                        "reward -26.96\n",
                        "True\n",
                        "reward -26.96\n",
                        "True\n",
                        "reward -26.96\n",
                        "True\n",
                        "reward -26.96\n",
                        "True\n",
                        "reward -26.96\n",
                        "True\n",
                        "reward -26.96\n",
                        "True\n",
                        "reward -26.96\n",
                        "True\n",
                        "reward -26.96\n",
                        "True\n",
                        "reward -27.96\n",
                        "True\n",
                        "reward -27.96\n",
                        "True\n",
                        "reward -27.96\n",
                        "True\n",
                        "reward -27.96\n",
                        "True\n",
                        "reward -27.96\n",
                        "True\n",
                        "reward -27.96\n",
                        "True\n",
                        "reward -27.96\n",
                        "True\n",
                        "reward -27.96\n",
                        "True\n",
                        "reward -27.96\n",
                        "True\n",
                        "reward -27.96\n",
                        "True\n",
                        "reward -28.96\n",
                        "True\n",
                        "reward -28.96\n",
                        "True\n",
                        "reward -28.96\n",
                        "True\n",
                        "reward -28.96\n",
                        "True\n",
                        "reward -28.96\n",
                        "True\n",
                        "reward -28.96\n",
                        "True\n",
                        "reward -28.96\n",
                        "True\n",
                        "reward -28.96\n",
                        "True\n",
                        "reward -28.96\n",
                        "True\n",
                        "reward -28.96\n",
                        "True\n",
                        "reward -29.96\n",
                        "True\n",
                        "reward -29.96\n",
                        "True\n",
                        "reward -29.96\n",
                        "True\n",
                        "reward -29.96\n",
                        "True\n",
                        "reward -29.96\n",
                        "True\n",
                        "reward -29.96\n",
                        "True\n",
                        "reward -29.96\n",
                        "True\n",
                        "reward -29.96\n",
                        "True\n",
                        "reward -29.96\n",
                        "True\n",
                        "reward -29.96\n",
                        "True\n",
                        "reward -30.96\n",
                        "True\n",
                        "reward -30.96\n",
                        "True\n"
                    ]
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[10], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreward\u001b[39m\u001b[38;5;124m\"\u001b[39m, reward)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(done)\n\u001b[0;32m---> 14\u001b[0m \u001b[43mtest_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
                        "File \u001b[0;32m~/Desktop/capstone/ECE113_capstone/./TetrisEnv/TetrisBattle/envs/tetris_env.py:98\u001b[0m, in \u001b[0;36mTetrisEnv.render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mviewer\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mviewer \u001b[38;5;241m=\u001b[39m rendering\u001b[38;5;241m.\u001b[39mSimpleImageViewer()\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mviewer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mviewer\u001b[38;5;241m.\u001b[39misopen\n",
                        "File \u001b[0;32m/opt/miniconda3/envs/ECE_113_Capstone_1/lib/python3.10/site-packages/gym/envs/classic_control/rendering.py:439\u001b[0m, in \u001b[0;36mSimpleImageViewer.imshow\u001b[0;34m(self, arr)\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39misopen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(arr\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou passed in an image with the wrong number shape\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    438\u001b[0m image \u001b[38;5;241m=\u001b[39m pyglet\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mImageData(\n\u001b[0;32m--> 439\u001b[0m     arr\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], arr\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43marr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtobytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, pitch\u001b[38;5;241m=\u001b[39marr\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m\n\u001b[1;32m    440\u001b[0m )\n\u001b[1;32m    441\u001b[0m texture \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mget_texture()\n\u001b[1;32m    442\u001b[0m gl\u001b[38;5;241m.\u001b[39mglTexParameteri(gl\u001b[38;5;241m.\u001b[39mGL_TEXTURE_2D, gl\u001b[38;5;241m.\u001b[39mGL_TEXTURE_MAG_FILTER, gl\u001b[38;5;241m.\u001b[39mGL_NEAREST)\n",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "source": [
                "# model = PPO.load('./train/best_model_20000')\n",
                "test_env = TetrisSingleEnv(obs_type=\"grid\")\n",
                "state = test_env.reset()\n",
                "i = 0\n",
                "\n",
                "while True:\n",
                "    if (i >= 1000):\n",
                "        env.close()\n",
                "        break\n",
                "    action, _ = model.predict(state)\n",
                "    state, reward, done, info = test_env.step(action)\n",
                "    print(\"reward\", reward)\n",
                "    print(done)\n",
                "    test_env.render()\n",
                "    i += 1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "state = naked_env.reset()\n",
                "print(state)\n",
                "print(model.predict(state))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.13"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
